{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "web = \"https://books.toscrape.com/\"\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "## we first of all create a function that scrape individual page\n",
    "def scrape_titles(driver):\n",
    "    products = driver.find_elements(By.XPATH, '//article[contains(@class, \"product_pod\")]')\n",
    "    titles = []\n",
    "    for product in products:\n",
    "        title_element = product.find_element(By.XPATH, './h3/a')\n",
    "        title = title_element.get_attribute('title')\n",
    "        titles.append(title)\n",
    "    return titles\n",
    "## thwn we run the driver to get web\n",
    "driver.get(web)\n",
    "\n",
    "## we create a list that will house all titles\n",
    "all_titles = []\n",
    "num_pages_to_scrape = 5  # Set the number of pages you want to scrape\n",
    "current_page = 1\n",
    "\n",
    "## we initiate a while loop to run for the number of page we want and extent the page scrapes to all_titles\n",
    "while current_page <= num_pages_to_scrape:\n",
    "    all_titles.extend(scrape_titles(driver))\n",
    "    try:\n",
    "        #Here we click the next button and wait a bit for page to load properly\n",
    "        next_button = driver.find_element(By.XPATH, '//li[@class=\"next\"]/a')\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "        current_page += 1\n",
    "    except:\n",
    "        break\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# here we print out our results\n",
    "for title in all_titles:\n",
    "    print(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Iterate over each quote element\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     quote \u001b[38;5;241m=\u001b[39m \u001b[43mquote_elements\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Find and click the author link\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         author_link \u001b[38;5;241m=\u001b[39m quote\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//a[contains(@href, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/author/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "web = \"https://quotes.toscrape.com//\"\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "def scrape_authors(driver):\n",
    "    products = driver.find_elements(By.XPATH, \"//div[contains(@class, 'author-details')\")\n",
    "    titles = []\n",
    "    birth = []\n",
    "    nation = []\n",
    "    desc = []\n",
    "    for product in products:\n",
    "        title_element = product.find_element(By.XPATH, \"./h3[@class='author-title']\").text\n",
    "        titles.append(title_element)\n",
    "        birh_element = product.find_element(By.XPATH, \".//span[@class='author-born-date']\").text\n",
    "        birth.append(birh_element)\n",
    "        nationality_element = product.find_element(By.XPATH, \".//span[@class='author-born-location']\").text\n",
    "        nation.append(nationality_element)\n",
    "        desc_element = product.find_element(By.XPATH, \".//div[@class='author-description']\").text\n",
    "        desc.append(desc_element)\n",
    "        \n",
    "    return titles, birth, nation , desc\n",
    "\n",
    "\n",
    "quote_elements = driver.find_elements(By.XPATH, '//div[@class=\"quote\"]')\n",
    "\n",
    "# Iterate over each quote element\n",
    "for i in range(5):\n",
    "    quote = quote_elements[i]\n",
    "    try:\n",
    "        # Find and click the author link\n",
    "        author_link = quote.find_element(By.XPATH, './/a[contains(@href, \"/author/\")]')\n",
    "        author_link.click()\n",
    "        \n",
    "        # Wait for the author page to load\n",
    "        time.sleep(2)  # Adjust if necessary\n",
    "        \n",
    "        # Extract author details\n",
    "        scrape_authors()\n",
    "        \n",
    "        # Navigate back to the main quotes page\n",
    "        driver.back()\n",
    "        \n",
    "        # Wait for the quotes page to load\n",
    "        time.sleep(2)  # Adjust if necessary\n",
    "\n",
    "        # Refresh the list of quote elements to avoid stale element reference\n",
    "        quote_elements = driver.find_elements(By.XPATH, '//div[@class=\"quote\"]')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        continue\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the main webpage\n",
    "web = \"https://quotes.toscrape.com/\"  # Replace with the actual URL\n",
    "driver.get(web)\n",
    "\n",
    "# Function to extract author details\n",
    "def extract_author_details():\n",
    "    author_name = driver.find_element(By.XPATH, '//div[@class=\"author-details\"]/h3[@class=\"author-title\"]').text\n",
    "    author_birth_date = driver.find_element(By.XPATH, '//div[@class=\"author-details\"]//span[@class=\"author-born-date\"]').text\n",
    "    author_birth_location = driver.find_element(By.XPATH, '//div[@class=\"author-details\"]//span[@class=\"author-born-location\"]').text\n",
    "    author_description = driver.find_element(By.XPATH, '//div[@class=\"author-details\"]//div[@class=\"author-description\"]').text\n",
    "    print(\"Author Name:\", author_name)\n",
    "    print(\"Born Date:\", author_birth_date)\n",
    "    print(\"Born Location:\", author_birth_location)\n",
    "    print(\"Author Description:\", author_description)\n",
    "\n",
    "# Set the number of quotes you want to scrape\n",
    "num_quotes_to_scrape = 5\n",
    "\n",
    "# Find all quote elements\n",
    "quote_elements = driver.find_elements(By.XPATH, '//div[@class=\"quote\"]')\n",
    "\n",
    "# Iterate over each quote element up to the desired number\n",
    "for i in range(num_quotes_to_scrape):\n",
    "    if i >= len(quote_elements):\n",
    "        break\n",
    "    quote = quote_elements[i]\n",
    "    try:\n",
    "        # Find and click the author link\n",
    "        author_link = quote.find_element(By.XPATH, './/a[contains(@href, \"/author/\")]')\n",
    "        author_link.click()\n",
    "        \n",
    "        # Wait for the author page to load\n",
    "        time.sleep(2)  # Adjust if necessary\n",
    "        \n",
    "        # Extract author details\n",
    "        extract_author_details()\n",
    "        \n",
    "        # Navigate back to the main quotes page\n",
    "        driver.back()\n",
    "        \n",
    "        # Wait for the quotes page to load\n",
    "        time.sleep(2)  # Adjust if necessary\n",
    "\n",
    "        # Refresh the list of quote elements to avoid stale element reference\n",
    "        quote_elements = driver.find_elements(By.XPATH, '//div[@class=\"quote\"]')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        continue\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the main webpage\n",
    "web = \"https://books.toscrape.com/\"  # Replace with the actual URL\n",
    "driver.get(web)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(2)  # Let's wait for 2 seconds to ensure the page is loaded fully\n",
    "\n",
    "def extracts():\n",
    "    Name = driver.find_element(By.XPATH, \".//div[@class='col-sm-6 product_main']/h1\").text\n",
    "    Category = driver.find_element(By.XPATH, './/ul[@class=\"breadcrumb\"]/li[last()-1]/a').text\n",
    "    price = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "    stock = driver.find_element(By.XPATH, \"//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    star_rating_element = driver.find_element(By.XPATH, \"//p[contains(@class, 'star-rating')]\")\n",
    "    star_rating = star_rating_element.get_attribute(\"class\").split()[-1]\n",
    "\n",
    "    desc = driver.find_element(By.XPATH, \"//div[@id='product_description']/following-sibling::p\").text\n",
    "\n",
    "    ## product information\n",
    "    ups = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-6]/td[last()]\").text\n",
    "    \n",
    "    Type = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-5]/td[last()]\").text\n",
    "\n",
    "    price_ex = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-4]/td[last()]\").text\n",
    "\n",
    "    price_in = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "\n",
    "    tax = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-2]/td[last()]\").text\n",
    "\n",
    "    avaliablity = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    review = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()]/td[last()]\").text\n",
    "\n",
    "    Bok_list = [Name,Category,price,stock,star_rating,desc, ups, Type, price_ex,price_in, tax ,avaliablity,review]\n",
    "\n",
    "\n",
    "    return Bok_list\n",
    "\n",
    "def page_extract():\n",
    "    all_books = []\n",
    "\n",
    "    #products = driver.find_element(By.XPATH, \"//article [@class='product_pod']\")\n",
    "    products = driver.find_elements(By.XPATH, \"//article[contains(@class='product_pod')]\"\n",
    "\n",
    "    for i in range(15):\n",
    "        book = products[i]\n",
    "        try:\n",
    "            book_link =  book.find_element(By.CSS_SELECTOR, \"a[href*= 'catalogue/')]\")\n",
    "            book_link.click()\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            \n",
    "\n",
    "            driver.back()\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            products = driver.find_element(By.XPATH, \"//article [@class='product_pod']\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        continue\n",
    "        \n",
    "    return len(products)\n",
    "\n",
    "#def pages_extract(num):\n",
    "    # current_page= 1\n",
    "    \n",
    "    # while current_page <= num:\n",
    "    #     x = []\n",
    "    #     x.append(page_extract())\n",
    "    #     try:\n",
    "    #         #Here we click the next button and wait a bit for page to load properly\n",
    "    #         next_button = driver.find_element(By.XPATH, '//li[@class=\"next\"]/a')\n",
    "    #         next_button.click()\n",
    "    #         time.sleep(2)  # Wait for the next page to load\n",
    "    #         current_page += 1\n",
    "    #     except:\n",
    "    #         break\n",
    "\n",
    "x = page_extract()\n",
    "#pages_extract(5)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the Attic', 'Poetry', '£51.77', 'In stock (22 available)', 'Three', \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\", 'a897fe39b1053632', 'Books', '£51.77', '£51.77', '£0.00', 'In stock (22 available)', '0']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "web = \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
    "service = Service(executable_path=\"chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "def extracts():\n",
    "    Name = driver.find_element(By.XPATH, \".//div[@class='col-sm-6 product_main']/h1\").text\n",
    "    Category = driver.find_element(By.XPATH, './/ul[@class=\"breadcrumb\"]/li[last()-1]/a').text\n",
    "    price = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "    stock = driver.find_element(By.XPATH, \"//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    star_rating_element = driver.find_element(By.XPATH, \"//p[contains(@class, 'star-rating')]\")\n",
    "    star_rating = star_rating_element.get_attribute(\"class\").split()[-1]\n",
    "\n",
    "    desc = driver.find_element(By.XPATH, \"//div[@id='product_description']/following-sibling::p\").text\n",
    "\n",
    "    ## product information\n",
    "    ups = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-6]/td[last()]\").text\n",
    "    \n",
    "    Type = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-5]/td[last()]\").text\n",
    "\n",
    "    price_ex = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-4]/td[last()]\").text\n",
    "\n",
    "    price_in = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "\n",
    "    tax = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-2]/td[last()]\").text\n",
    "\n",
    "    avaliablity = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    review = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()]/td[last()]\").text\n",
    "\n",
    "    Bok_list = [Name,Category,price,stock,star_rating,desc, ups, Type, price_ex,price_in, tax ,avaliablity,review]\n",
    "\n",
    "\n",
    "    return Bok_list\n",
    "\n",
    "\n",
    "driver.get(web)\n",
    "\n",
    "x = extracts()\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the driver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Assuming products is a list of WebElements, for example:\n",
    "products = driver.find_elements(By.CLASS_NAME, 'product_pod')  # Or the appropriate locator for your product elements\n",
    "\n",
    "# Ensure products is a list and contains WebElements\n",
    "if isinstance(products, list) and all(isinstance(product, webdriver.remote.webelement.WebElement) for product in products):\n",
    "    for i in range(min(len(products), 20)):  # To avoid IndexError if products has fewer than 20 elements\n",
    "        try:\n",
    "            book = products[i]\n",
    "            book_link = book.find_element(By.XPATH, \".//h3//a[contains(@href, 'catalogue/')]\")\n",
    "            book_link.click()\n",
    "            # Additional code to handle after clicking the book link\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(\"The 'products' variable is not a list of WebElements.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize the driver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the target URL\n",
    "driver.get('https://books.toscrape.com/')\n",
    "\n",
    "# Wait for the product elements to be present\n",
    "\n",
    "\n",
    "def extracts():\n",
    "    Name = driver.find_element(By.XPATH, \".//div[@class='col-sm-6 product_main']/h1\").text\n",
    "    Category = driver.find_element(By.XPATH, './/ul[@class=\"breadcrumb\"]/li[last()-1]/a').text\n",
    "    price = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "    stock = driver.find_element(By.XPATH, \"//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    star_rating_element = driver.find_element(By.XPATH, \"//p[contains(@class, 'star-rating')]\")\n",
    "    star_rating = star_rating_element.get_attribute(\"class\").split()[-1]\n",
    "\n",
    "    desc = driver.find_element(By.XPATH, \"//div[@id='product_description']/following-sibling::p\").text\n",
    "\n",
    "    ## product information\n",
    "    ups = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-6]/td[last()]\").text\n",
    "    \n",
    "    Type = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-5]/td[last()]\").text\n",
    "\n",
    "    price_ex = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-4]/td[last()]\").text\n",
    "\n",
    "    price_in = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "\n",
    "    tax = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-2]/td[last()]\").text\n",
    "\n",
    "    avaliablity = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    review = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()]/td[last()]\").text\n",
    "\n",
    "    Bok_list = [Name,Category,price,stock,star_rating,desc, ups, Type, price_ex,price_in, tax ,avaliablity,review]\n",
    "\n",
    "\n",
    "    return Bok_list\n",
    "\n",
    "def page_extract():\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'product_pod')))\n",
    "    products = driver.find_elements(By.CLASS_NAME, 'product_pod')\n",
    "        # Find all product elements\n",
    "    all_books = []\n",
    "\n",
    "    # Ensure products is a list of WebElements\n",
    "    if isinstance(products, list) and all(isinstance(product, webdriver.remote.webelement.WebElement) for product in products):\n",
    "        max_products = min(len(products), 20)  # Avoid IndexError if fewer than 20 elements\n",
    "        \n",
    "        for i in range(max_products):\n",
    "            try:\n",
    "                # Re-locate the products list in each iteration to handle dynamic page updates\n",
    "                products = driver.find_elements(By.CLASS_NAME, 'product_pod')\n",
    "                \n",
    "                # Ensure the list has enough elements to avoid IndexError\n",
    "                if i >= len(products):\n",
    "                    break\n",
    "                \n",
    "                book = products[i]\n",
    "                book_link = book.find_element(By.XPATH, \".//h3//a\")\n",
    "                book_link.click()\n",
    "                \n",
    "                all_books.append(extracts())\n",
    "\n",
    "                # Navigate back to the product list page\n",
    "                driver.back()\n",
    "                \n",
    "                # Wait for the product list to be present again\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'product_pod')))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred at index {i}: {e}\")\n",
    "    else:\n",
    "        print(\"The 'products' variable is not a list of WebElements.\")\n",
    "\n",
    "    return all_books\n",
    "\n",
    "    # Close the driver\n",
    "\n",
    "\n",
    "page_extract()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Initialize the driver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the target URL\n",
    "driver.get('https://books.toscrape.com/')\n",
    "\n",
    "\n",
    "# Wait for the product elements to be present\n",
    "\n",
    "\n",
    "def extracts():\n",
    "    Name = driver.find_element(By.XPATH, \".//div[@class='col-sm-6 product_main']/h1\").text\n",
    "    Category = driver.find_element(By.XPATH, './/ul[@class=\"breadcrumb\"]/li[last()-1]/a').text\n",
    "    price = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "    stock = driver.find_element(By.XPATH, \"//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    star_rating_element = driver.find_element(By.XPATH, \"//p[contains(@class, 'star-rating')]\")\n",
    "    star_rating = star_rating_element.get_attribute(\"class\").split()[-1]\n",
    "\n",
    "    desc = driver.find_element(By.XPATH, \"//div[@id='product_description']/following-sibling::p\").text\n",
    "\n",
    "    ## product information\n",
    "    ups = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-6]/td[last()]\").text\n",
    "    \n",
    "    Type = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-5]/td[last()]\").text\n",
    "\n",
    "    price_ex = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-4]/td[last()]\").text\n",
    "\n",
    "    price_in = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-3]/td[last()]\").text\n",
    "\n",
    "    tax = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-2]/td[last()]\").text\n",
    "\n",
    "    avaliablity = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()-1]/td[last()]\").text\n",
    "\n",
    "    review = driver.find_element(By.XPATH, \".//table[@class='table table-striped']/tbody/tr[last()]/td[last()]\").text\n",
    "\n",
    "    Bok_list = [Name,Category,price,stock,star_rating,desc, ups, Type, price_ex,price_in, tax ,avaliablity,review]\n",
    "\n",
    "\n",
    "    return Bok_list\n",
    "\n",
    "def page_extract():\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'product_pod')))\n",
    "    products = driver.find_elements(By.CLASS_NAME, 'product_pod')\n",
    "    # Find all product elements\n",
    "    all_books = []\n",
    "\n",
    "    # Ensure products is a list of WebElements\n",
    "    if isinstance(products, list) and all(isinstance(product, webdriver.remote.webelement.WebElement) for product in products):\n",
    "        max_products = min(len(products), 20)  # Avoid IndexError if fewer than 20 elements\n",
    "        \n",
    "        for i in range(max_products):\n",
    "            try:\n",
    "                # Re-locate the products list in each iteration to handle dynamic page updates\n",
    "                products = driver.find_elements(By.CLASS_NAME, 'product_pod')\n",
    "                \n",
    "                # Ensure the list has enough elements to avoid IndexError\n",
    "                if i >= len(products):\n",
    "                    break\n",
    "                \n",
    "                book = products[i]\n",
    "                book_link = book.find_element(By.XPATH, \".//h3//a\")\n",
    "                book_link.click()\n",
    "                \n",
    "                all_books.append(extracts())\n",
    "\n",
    "                # Navigate back to the product list page\n",
    "                driver.back()\n",
    "                \n",
    "                # Wait for the product list to be present again\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'product_pod')))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred at index {i}: {e}\")\n",
    "    else:\n",
    "        print(\"The 'products' variable is not a list of WebElements.\")\n",
    "\n",
    "    return all_books\n",
    "\n",
    "\n",
    "def pages_extract(num):\n",
    "    current_page = 0\n",
    "    num_pages_to_scrape = num\n",
    "    all_bok = []\n",
    "    while current_page <= num_pages_to_scrape:\n",
    "        try:\n",
    "            x = page_extract()\n",
    "            all_bok.append(x)\n",
    "            #Here we click the next button and wait a bit for page to load properly\n",
    "            next_button = driver.find_element(By.XPATH, '//li[@class=\"next\"]/a')\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            current_page += 1\n",
    "        except:\n",
    "            break\n",
    "    return all_bok\n",
    "\n",
    "y = pages_extract(4)\n",
    "\n",
    "print(len(y))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Dance of the Wicked\n",
      "\n",
      "First paragraph: Dance of the Wicked is heavy metal band Sister Sin's first studio album, released 2003. Recorded and produced in 2003 by Jan Jan at Triton Studios Gothenburg. Released and Distributed by Sleaszy Rider Records & EMI Greece Distribution.\n",
      "\n",
      "\n",
      "Scraped data from: https://en.wikipedia.org/wiki/Dance_of_the_Wicked\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def fetch_random_wikipedia_page():\n",
    "    try:\n",
    "        # URL for a random Wikipedia page\n",
    "        random_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
    "        \n",
    "        # Send a GET request to the random URL\n",
    "        response = requests.get(random_url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to fetch page, status code: {response.status_code}\")\n",
    "        \n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the title of the page\n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "        \n",
    "        # Extract the first paragraph of the page\n",
    "        first_paragraph = soup.find('p').text\n",
    "        \n",
    "        # Print the title and first paragraph\n",
    "        print(f\"Title: {title}\\n\")\n",
    "        print(f\"First paragraph: {first_paragraph}\\n\")\n",
    "        \n",
    "        # Optionally, return the data\n",
    "        return {\n",
    "            'title': title,\n",
    "            'first_paragraph': first_paragraph,\n",
    "            'url': response.url\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Fetch and display a random Wikipedia page\n",
    "page_data = fetch_random_wikipedia_page()\n",
    "\n",
    "# If needed, do something with the data\n",
    "if page_data:\n",
    "    print(f\"Scraped data from: {page_data['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
